package com.info.kafka_stream_demo

import java.util.{Date, Properties}

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}

import scala.util.Random

/**
 * 
 * 
 * 
 * 
 *  **/
object ScalaProducerExample extends App {
   def main(args: Array[String]): Unit = {
    
  
  val events = 15;
  val topic = "test_topic"
  val brokers = "xxx.xxx.0.xxx:9092";
  val rnd = new Random()
 val props = new Properties();
 //props.put("security.protocol", "PLAINTEXT");
 props.put("bootstrap.servers", "xxx.xxx.0.xxx:9092"); //The bootstrap.servers config accepts a list of host/port pairs that are used for establishing
 props.put("acks", "all"); //acknowledgement record complete ISR
 props.put("retries", "0"); // no of retries
 props.put("batch.size", "16384"); // batch size
 props.put("linger.ms", "1");
 props.put("buffer.memory", "33554432"); // limit the memory
 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
 props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

 //initiate the producer
 val producer= new KafkaProducer[String,String](props);
 // send function return the record meta data
 val record = producer.send(new ProducerRecord[String, String]("test_topic", Integer.toString(1), Integer.toString(1))).get;
 //To make writes synchronous, we can just wait on the returned future by invoking the get()
//function on the future. If we invoke get() on the future, our application will block
 producer.close();